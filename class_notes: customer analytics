# datacamp
notes from class

PYTHON
======
MEDITATION APP EXERCISE

Customer analytics

#join 2 tables

# Import pandas 
import pandas as pd

# Load the customer_data
customer_data = pd.read_csv("customer_data.csv")

# Load the app_purchases
app_purchases = pd.read_csv("inapp_purchases.csv")

# Print the columns of customer data
print(customer_data.columns)

# Print the columns of app_purchases
print(app_purchases.columns)

# Merge 2 tables
# Merge on the 'uid' and 'date' field
uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')

# Examine the results 
print(uid_date_combined_data.head())
print(len(uid_date_combined_data))

# Group the data 
grouped_purchase_data = purchase_data.groupby(['device', 'gender'])

# Aggregate the data
purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})

# Examine the results
print(purchase_summary)

# Compute max_purchase_date
max_purchase_date = current_date - timedelta(days=28)

# Filter to only include users who registered before our max date
purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]

# Filter to contain only purchases within the first 28 days of registration
purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <= 
                        purchase_data_filt.reg_date + timedelta(days=28))]

# Output the mean price paid per purchase
print(purchase_data_filt.price.mean())

# Parsing dates: https://strftime.org/

# Week one converstion rate by day
import pandas as pd
from datetime import timedelta

# The maximum date in our dataset
current_date = pd.to_datetime('2018-03-17')

# Limit to users who have had a week to subscribe
max_lapse_date = current_date - timedelta (days=7)
conv_sub_data = sub_data_demo[sub_data_demo.lapse_date < max_lapse_date]

# Calculate how many days it took the user to subscribe. Lapse date is the first day a user is eligible to subscribe
conv_sub_date['sub_time'] = (conv_sub_data.subscription_date - conv_sub_data.lapse_date.dt.days)

# Find the conversion rate for each daily cohort
conversion_data = conv_sub_data.groupby(by=['lapse_date'], as_index=False).agg({'sub_time': [gc7]})

# Break out our conversion rate by country, pivot data to graph time series 
reformatted_cntry_data = pd.pivot_table(
  conversion_data # data frame to reshape
  values = ['sub_time'], # our primary value
  columns = ['country], # what to break out
  index = ['reg_date'], # the value to use as rows
  fill_value = 0)
 
reformatted_cntry_data.plot (
  x = 'reg_date',
  y = ['BRA', 'FRA', 'DEU', 'TUR', 'USA', 'CAN']
)
  
plt.show()

# Understanding trends
# Subscribers per day

# Find the days to subscribe of our loaded usa subs data set 
usa_subscriptions['sub_day']= (usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days

# Filter out those who subscribed in the past week
usa_subscriptions = usa_subscriptions [usa_subscriptions.sub_day <=7]

# Find the total subscribers per day
usa_subscriptions = usa_subscriptions.groupby(by=['sub_date'], as_index = False).agg({'subs':['sum']})

# plot USA subscribers per day
usa_subscriptions.plot(x='sub_date', y='subs')
plt.show()

# Weekly seasonality: trendse following day of the week - perhaps users more likely to subscribe on weekend; seasonality can show price change?

# Correcting for seasonality with trailing averages

# Trailing Average: smoothing technique that averages over a lagging window

# Reveal hidden trends by smoothing out seasonality
# Average across period of seasonality
# 7-day window to smooth weekly seasonality

